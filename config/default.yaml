# 默认配置文件，展示 CLI 参数的典型取值。  # 对整个文件的说明。
input: ./samples  # 输入音频目录默认指向 samples。
out_dir: ./out  # 转写输出目录默认位于仓库内的 out。
backend_cli_default: dummy  # CLI 层的默认后端保持 dummy 以兼容早期轮次。
language: auto  # 默认自动检测语言。
segments_json: true  # 默认导出段级 JSON。
overwrite: false  # 默认不覆盖已有结果。
num_workers: 1  # 默认使用单线程处理。
dry_run: false  # 默认真实落盘。
verbose: false  # 默认关闭详细日志。
runtime:  # 新增运行时默认设置集合。
  backend: faster-whisper  # 推荐的真实后端。
  language: auto  # 转写语言默认自动检测，可被 CLI 覆盖。
  segments_json: true  # 是否输出段级 JSON。
  overwrite: false  # 是否覆盖已有结果。
  compute_type: auto  # faster-whisper compute_type，可设 int8/int8_float16/float16/float32。
  device: auto  # 设备选择，auto 会在 GPU 可用时自动切换。
  beam_size: 5  # 默认 beam search 宽度，1 可换取更快速度。
  temperature: 0.0  # 默认温度，0 表示贪心/beam 搜索。
  vad_filter: false  # 是否启用 VAD 过滤，Round 7 仅记录。
  chunk_length_s: null  # 可选切块长度（秒），null 使用库默认值。
  best_of: null  # 采样模式下的候选数，beam search 时忽略。
  patience: null  # beam search 提前停止阈值，可保持 null。
  whisper_cpp:  # whisper.cpp 后端的运行默认值。
    executable_path: ""  # 可执行文件路径，安装脚本会尝试写入或提示用户手动填写。
    model_path: ""  # GGML/GGUF 模型文件路径，建议存放在 models/whisper.cpp/ 下。
    threads: 0  # 线程数，0 表示由 whisper.cpp 自动根据硬件选择。
    beam_size: 5  # beam search 宽度，与 faster-whisper 保持一致。
    temperature: 0.0  # 温度参数，0 表示贪心策略。
    max_len: null  # 输出最大 token 数，null 表示使用默认。
    prompt: null  # 初始提示词，可在 CLI 中覆盖。
    print_progress: false  # 是否打印进度条，默认关闭以减少日志噪音。
    timeout_sec: null  # 子进程调用超时时间，null 表示不设置超时。
backend:  # 后端下载相关默认值聚合于此字典。
  default: faster-whisper  # 推荐的真实后端为 faster-whisper。
model:  # 模型配置相关键集合。
  default: medium  # 默认下载 medium 规格，可选 tiny|base|small|medium|large-v3。
cache_dir: .cache/  # 仓库内缓存目录默认指向 .cache/，可被脚本覆盖。
models_dir: ~/.cache/asrprogram/models/  # 默认将模型缓存在用户目录下的专用路径。
download:  # 下载相关默认参数集合。
  mirrors:  # 下载镜像列表，脚本会按顺序依次尝试。
    - https://huggingface.co  # 官方主仓库，提供最新模型文件。
    - https://hf-mirror.com  # 备用镜像，适合部分地区加速访问。
  timeout_sec: 60  # 单次网络请求的默认超时时长（秒）。
  retries: 3  # 每个文件的最大重试次数。
